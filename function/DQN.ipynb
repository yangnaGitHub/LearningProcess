{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "&nbsp;Q-learning更新规则: \n",
    "$$Q_r=reward+\\gamma max_{a_n}Q(s_n,a_n),现实=当前的奖励和未来奖励的衰减,\\gamma是对远处价值重视度的体现$$\n",
    "$$Q(S_1)=reward_2+\\gamma Q(S_2), Q(S_2)=reward_3+\\gamma Q(S_3)==>Q(S_1)=reward_2+\\gamma reward_3+\\gamma^2reward_4+\\cdots,自己和子孙产生的价值$$\n",
    "$$Q(s,a)<==Q(s,a)+\\alpha[Q_r-Q(s,a)], \\alpha是学习率,\\gamma是衰减因子$$\n",
    "&nbsp;Sarsa更新规则:\n",
    "$$\\delta=reward+\\gamma Q(s_n,a_n)-Q(s,a),\\gamma是衰减因子,a_n是选择的$$\n",
    "$$Q(s,a)<==Q(s,a)+\\alpha \\delta,\\alpha是学习率$$\n",
    "&nbsp;Sarsa-lambda更新规则:\n",
    "$$E(s,a)=E(s,a)+1得到reward路途中不可或缺的一环,Q(s,a)<==Q(s,a)+\\alpha \\delta E(s,a),\\alpha是学习率$$\n",
    "$$E(s,a)=\\gamma \\lambda E(s,a),\\lambda后向观测,时间衰减eligibility　trace的值,离获取reward越远的步,他的不可或缺性越小$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
