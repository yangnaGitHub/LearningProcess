# -*- coding: utf-8 -*-
"""
Created on Fri Mar 03 16:08:44 2017

@author: natasha1_Yang
"""

#Adaboost:把多个简单的分类器结合起来形成个复杂的分类器
#Adaboost是一种迭代算法,针对同一训练集训练不同的分类器(弱分类器),结合这些弱分类器,构造一个强分类器
#Adaboost算法提供的是框架,不用担心过拟合
#算法思想:给每一个训练数据添加“权值”(平均分配)
#    提高那些被前一轮弱分类器错误分类样本的权值(权值的加大而受到后一轮弱分类器的更大关注),降低那些被正确分类样本的权值
#    给每一个弱分类器添加“权值”,采取多数表决的方法组合弱分类器
#    加大分类误差率小的弱分类器的权值,减少分类误差率大的弱分类器的权值
#order  1  2  3  4  5  6  7  8  9  10
#  x    0  1  2  3  4  5  6  7  8  9
#  y    1  1  1 -1 -1 -1  1  1  1 -1
#m=1, Dm = (Wm1, Wm2,...,Wm10) ==> D1 = (W11, W12,...,W110) Wmi = 0.1
#V取1.5~9.5计算取的每点的误差率
#V取2.5的时候,误分类的点为6, 7, 8,其权值和为e1 = 0.3,误差率最低
#    得到G1(x) = 1 (x < 2.5)
#               -1 (x > 2.5)
#    计算G1(x)的系数a1 = (1/2)log[(1-e1)/e1] = 0.4236
#    更新权重分布D2 = (W21, W22,...,W210)
#    W2i = (W1i/Z1)exp(-a1YiG1(Xi))
#==>D2 = (0.0715, 0.0715, 0.0715, 0.0715,0.0715, 0.0715, 0.1666, 0.1666, 0.1666, 0.0715) ==> f1(x) = 0.4236G1(x)
#同理在做上面的步骤得到看误分类的点
#误分类的点最终为0确定最终分类器
#前向分布算法:学习的是加法模型,从前向后每一步只学习一个基函数及其系数,然后逐步逼近优化
#提升树是以决策树为弱分类器的提升方法
#提升树:加法模型 + 前向分布算法 + CART树(基函数)
#平方误差损失函数的回归问题,指数损失函数的分类问题,一般损失函数的一般决策问题